DatasetDict({
    train: Dataset({
        features: ['problem', 'answer', 'type'],
        num_rows: 19952
    })
})
DatasetDict({
    train: Dataset({
        features: ['problem', 'answer', 'type', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],
        num_rows: 19952
    })
})
Training set size: 15961
Validation set size: 3991
{'problem': "Here's a way to construct a list containing every positive rational number:\n\nBuild a binary tree where each node is a rational and the root is `1/1`, with the following rules for creating the nodes below:\n* The value of the left-hand node below `a/b` is `a/a+b` or `a+b/b`\n* The value of the right-hand node below `a/b` is `a/a+b` or `a+b/b`\n\nSo the tree will look like this:\n\n```\n                       1/1\n                  /           \\ \n            1/2                  2/1\n           /    \\              /     \\\n       1/3        3/2        2/3       3/1\n      /   \\      /   \\      /   \\     /   \\\n   1/4    4/3  3/5   5/2  2/5   5/3  3/4   4/1\n \n ...\n```\n\nNow traverse the tree, breadth first, to get a list of rationals.\n\n```\n[ 1/1, 1/2, 2/1, 1/3, 3/2, 2/3, 3/1, 1/4, 4/3, 3/5, 5/2, .. ]\n```\n\nEvery positive rational will occur, in its reduced form, exactly once in the list, at a finite index.\n\n```if:haskell\nIn the kata, we will use tuples of type `(Integer, Integer)` to represent rationals, where `(a, b)` represents `a / b`\n```\n```if:javascript\nIn the kata, we will use tuples of type `[ Number, Number ]` to represent rationals, where `[a,b]` represents `a / b`\n```\n\nUsing this method you could create an infinite list of tuples:\n\nmatching the list described above:\n\nHowever, constructing the actual list is too slow for our purposes. Instead, study the tree above, and write two functions:\n\nFor example:\n", 'answer': '1. What is the definition of "reduced form" for a rational number?\n2. What is the order in which the nodes are traversed in the breadth-first traversal?\n3. What is the expected output format for the list of rationals?', 'type': 'Ambiguous', 'input_ids': [18528, 35361, 35329, 1643, 2362, 1676, 5674, 1643, 2769, 9684, 2274, 4739, 13301, 2697, 35371, 30, 7886, 1643, 9786, 6420, 2181, 2496, 5746, 1709, 1643, 13301, 1679, 1651, 7727, 1709, 2794, 35381, 35376, 35381, 10894, 1749, 1651, 3024, 6111, 1713, 6684, 1651, 9660, 3902, 35371, 1396, 35386, 1751, 2808, 1677, 1651, 3051, 35347, 5429, 5746, 3902, 2794, 35324, 35376, 35340, 35425, 1709, 2794, 35324, 35376, 35324, 35402, 35340, 35425, 1801, 2794, 35324, 35402, 35340, 35376, 35340, 35425, 1396, 35386, 1751, 2808, 1677, 1651, 2431, 35347, 5429, 5746, 3902, 2794, 35324, 35376, 35340, 35425, 1709, 2794, 35324, 35376, 35324, 35402, 35340, 35425, 1801, 2794, 35324, 35402, 35340, 35376, 35340, 35425, 30, 7407, 1651, 6420, 1891, 2250, 2091, 1820, 35371, 30, 18414, 35425, 1396, 34, 33, 31, 35321, 35381, 35376, 35381, 1396, 34, 32, 31, 35376, 33, 31, 1710, 35321, 1396, 34, 35381, 35376, 35385, 34, 32, 31, 35385, 35376, 35381, 1396, 33, 31, 2539, 32, 35346, 34, 31, 35376, 32, 1710, 1396, 32, 31, 35321, 35381, 35376, 35399, 33, 35399, 35376, 35385, 33, 35385, 35376, 35399, 32, 31, 35321, 35399, 35376, 35381, 1396, 32, 31, 35376, 31, 1710, 32, 31, 35376, 31, 1710, 32, 31, 35376, 31, 1710, 32, 2539, 31, 1710, 1396, 31, 35321, 35381, 35376, 35404, 32, 35404, 35376, 35399, 31, 35399, 35376, 35405, 31, 35321, 35405, 35376, 35385, 31, 35385, 35376, 35405, 31, 35321, 35405, 35376, 35399, 31, 35399, 35376, 35404, 31, 35321, 35404, 35376, 35381, 1396, 35321, 1396, 4240, 1396, 18414, 35425, 30, 11664, 4691, 4820, 1651, 6420, 35343, 34816, 2168, 35343, 1676, 2041, 1643, 2769, 1677, 1752, 15179, 35342, 30, 18414, 35425, 1396, 35395, 35321, 35381, 35376, 35381, 35343, 35321, 35381, 35376, 35385, 35343, 35321, 35385, 35376, 35381, 35343, 35321, 35381, 35376, 35399, 35343, 35321, 35399, 35376, 35385, 35343, 35321, 35385, 35376, 35399, 35343, 35321, 35399, 35376, 35381, 35343, 35321, 35381, 35376, 35404, 35343, 35321, 35404, 35376, 35399, 35343, 35321, 35399, 35376, 35405, 35343, 35321, 35405, 35376, 35385, 35343, 11634, 7077, 1396, 18414, 35425, 30, 18478, 4739, 13301, 1891, 5437, 35343, 1672, 2110, 7197, 2386, 35343, 5210, 3576, 1672, 1651, 2769, 35343, 1785, 1643, 7184, 5418, 35342, 30, 18414, 35425, 1740, 35371, 35331, 3251, 1811, 1396, 2582, 1651, 1823, 2061, 35343, 1800, 1891, 2243, 9962, 3620, 1677, 3094, 2794, 35354, 15681, 35343, 23229, 35353, 35425, 1676, 3478, 1752, 15179, 35343, 2181, 2794, 35354, 35324, 35343, 1665, 35353, 35425, 8368, 2794, 35324, 2539, 1665, 35425, 1396, 18414, 35425, 1396, 18414, 35425, 1740, 35371, 20769, 1396, 2582, 1651, 1823, 2061, 35343, 1800, 1891, 2243, 9962, 3620, 1677, 3094, 2794, 35395, 13198, 35343, 13198, 7077, 35425, 1676, 3478, 1752, 15179, 35343, 2181, 2794, 35395, 35324, 35343, 35340, 35396, 35425, 8368, 2794, 35324, 2539, 1665, 35425, 1396, 18414, 35425, 30, 30604, 1820, 3108, 1735, 2216, 3797, 1748, 13584, 2769, 1677, 9962, 3620, 35371, 30, 13041, 1670, 1651, 2769, 5369, 3433, 35371, 30, 19698, 35343, 27566, 1651, 6447, 2769, 1709, 2745, 5154, 1713, 2046, 8803, 35342, 7906, 35343, 3695, 1651, 6420, 3433, 35343, 1679, 4612, 2164, 5248, 35371, 30, 5468, 3415, 35371, 1396, 35381, 35342, 2975, 1709, 1651, 6921, 1677, 1784, 20112, 2458], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 35381, 35342, 2975, 1709, 1651, 6921, 1677, 1784, 20112, 2458]}
{'problem': 'Greg has a weighed directed graph, consisting of n vertices. In this graph any pair of distinct vertices has an edge between them in both directions. Greg loves playing with the graph and now he has invented a new game:  The game consists of n steps.  On the i-th step Greg removes vertex number x_{i} from the graph. As Greg removes a vertex, he also removes all the edges that go in and out of this vertex.  Before executing each step, Greg wants to know the sum of lengths of the shortest paths between all pairs of the remaining vertices. The shortest path can go through any remaining vertex. In other words, if we assume that d(i, v, u) is the shortest path between vertices v and u in the graph that formed before deleting vertex x_{i}, then Greg wants to know the value of the following sum: $\\sum_{v, u, v \\neq u} d(i, v, u)$. \n\nHelp Greg, print the value of the required sum before each step.\n\n\n-----Input-----\n\nThe first line contains integer n (1 ≤ n ≤ 500) — the number of vertices in the graph.\n\nNext n lines contain n integers each — the graph adjacency matrix: the j-th number in the i-th line a_{ij} (1 ≤ a_{ij} ≤ 10^5, a_{ii} = 0) represents the weight of the edge that goes from vertex i to vertex j.\n\nThe next line contains n distinct integers: x_1, x_2, ..., x_{n} (1 ≤ x_{i} ≤ n) — the vertices that Greg deletes.\n\n\n-----Output-----\n\nPrint n integers — the i-th number equals the required sum before the i-th step.\n\nPlease, do not use the %lld specifier to read or write 64-bit integers in C++. It is preferred to use the cin, cout streams of the %I64d specifier.\n\n\n-----Examples-----\nInput\n1\n0\n1\n\nOutput\n0 \nInput\n2\n0 5\n4 0\n1 2\n\nOutput\n9 0 \nInput\n4\n0 3 1 1\n6 0 400 1\n2 4 0 1\n1 1 1 0\n4 1 2 3\n\nOutput\n17 23 404 0', 'answer': "import sys\nfrom array import array  # noqa: F401\n\nn = int(input())\nmatrix = [array('i', list(map(int, input().split()))) for _ in range(n)]\naa = tuple([int(x) - 1 for x in input().split()])\nans = [''] * n\n\nfor i in range(n-1, -1, -1):\n    x = aa[i]\n\n    for a in range(n):\n        for b in range(n):\n            if matrix[a][b] > matrix[a][x] + matrix[x][b]:\n                matrix[a][b] = matrix[a][x] + matrix[x][b]\n\n    val, overflow = 0, 0\n    for a in aa[i:]:\n        for b in aa[i:]:\n            val += matrix[a][b]\n        if val > 10**9:\n            overflow += 1\n            val -= 10**9\n\n    ans[i] = str(10**9 * overflow + val)\n\nprint(' '.join(ans))\n", 'type': 'Original', 'input_ids': [35380, 3374, 1920, 1643, 33568, 9905, 5060, 35343, 16146, 1677, 1681, 10685, 35342, 1886, 1820, 5060, 2050, 5739, 1677, 8640, 10685, 1920, 1748, 6009, 2517, 2075, 1672, 2607, 12159, 35342, 12470, 10867, 5341, 1749, 1651, 5060, 1679, 2353, 1731, 1920, 24821, 1643, 2049, 3145, 35371, 31, 2532, 3145, 9520, 1677, 1681, 6418, 35342, 31, 6047, 1651, 1994, 35347, 1797, 4564, 12470, 29619, 9828, 2697, 2775, 1887, 35325, 35349, 1829, 1651, 5060, 35342, 2336, 12470, 29619, 1643, 9828, 35343, 1731, 2043, 29619, 1857, 1651, 9715, 1726, 1910, 1672, 1679, 1936, 1677, 1820, 9828, 35342, 31, 27353, 30714, 2496, 4564, 35343, 12470, 6093, 1676, 2205, 1651, 3404, 1677, 19477, 1677, 1651, 29081, 13106, 2517, 1857, 10530, 1677, 1651, 8000, 10685, 35342, 1751, 29081, 3996, 1852, 1910, 2286, 2050, 8000, 9828, 35342, 1886, 2037, 4234, 35343, 1963, 1800, 6216, 1726, 1659, 35354, 35325, 35343, 1730, 35343, 1725, 35353, 1709, 1651, 29081, 3996, 2517, 10685, 1730, 1679, 1725, 1672, 1651, 5060, 1726, 8769, 2478, 21641, 2384, 9828, 2775, 1887, 35325, 2270, 2220, 12470, 6093, 1676, 2205, 1651, 2808, 1677, 1651, 3024, 3404, 35371, 1924, 3036, 1887, 35344, 35343, 1725, 35343, 1730, 1710, 8829, 1725, 35349, 1659, 35354, 35325, 35343, 1730, 35343, 1725, 4282, 35321, 30, 29278, 12470, 35343, 4471, 1651, 2808, 1677, 1651, 4193, 3404, 2478, 2496, 4564, 35342, 30, 1396, 28359, 9299, 28359, 30, 2532, 2168, 2793, 5977, 11497, 1681, 1737, 35381, 35321, 35764, 1681, 35321, 35764, 35321, 528, 35353, 2310, 1651, 2697, 1677, 10685, 1672, 1651, 5060, 35342, 30, 15020, 1681, 4921, 5530, 1681, 22307, 2496, 2310, 1651, 5060, 5370, 1694, 2999, 5848, 35371, 1651, 1834, 35347, 1797, 2697, 1672, 1651, 1994, 35347, 1797, 2793, 1643, 1887, 2021, 35349, 1737, 35381, 35321, 35764, 1643, 1887, 2021, 35349, 35321, 35764, 35321, 38, 35388, 35405, 35343, 1643, 1887, 4582, 35349, 1813, 35321, 35384, 35353, 8368, 1651, 4911, 1677, 1651, 6009, 1726, 5172, 1829, 9828, 1994, 1676, 9828, 1834, 35342, 30, 2532, 2865, 2793, 5977, 1681, 8640, 22307, 35371, 2775, 35356, 35381, 35343, 2775, 35356, 35385, 35343, 4240, 35343, 2775, 1887, 35327, 35349, 1737, 35381, 35321, 35764, 2775, 1887, 35325, 35349, 35321, 35764, 1681, 35353, 2310, 1651, 10685, 1726, 12470, 14962, 1656, 35342, 30, 1396, 28359, 11547, 28359, 30, 14859, 1681, 22307, 2310, 1651, 1994, 35347, 1797, 2697, 21357, 1651, 4193, 3404, 2478, 1651, 1994, 35347, 1797, 4564, 35342, 30, 16123, 35343, 1858, 1821, 2243, 1651, 4055, 35330, 1793, 2457, 7351, 1676, 2523, 1801, 4612, 35321, 92, 35347, 4481, 22307, 1672, 1718, 4778, 35342, 2005, 1709, 13412, 1676, 2243, 1651, 10175, 35343, 29536, 20763, 1677, 1651, 4055, 35351, 92, 35332, 2457, 7351, 35342, 30, 1396, 28359, 3358, 13999, 28359, 1396, 9299, 1396, 35381, 1396, 35384, 1396, 35381, 30, 11547, 1396, 35384, 35321, 1396, 9299, 1396, 35385, 1396, 35384, 35321, 35405, 1396, 35404, 35321, 35384, 1396, 35381, 35321, 35385, 30, 11547, 1396, 35419, 35321, 35384, 35321, 1396, 9299, 1396, 35404, 1396, 35384, 35321, 35399, 35321, 35381, 35321, 35381, 1396, 35408, 35321, 35384, 35321, 428, 35321, 35381, 1396, 35385, 35321, 35404, 35321, 35384, 35321, 35381, 1396, 35381, 35321, 35381, 35321, 35381, 35321, 35384, 1396, 35404, 35321, 35381, 35321, 35385], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}
trainable params: 13107200 || all params: 7263391744 || trainable%: 0.18045563921052912
compiling the model
