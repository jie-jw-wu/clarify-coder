DatasetDict({
    train: Dataset({
        features: ['problem', 'answer', 'type'],
        num_rows: 19952
    })
})
DatasetDict({
    train: Dataset({
        features: ['problem', 'answer', 'type', 'input_ids', 'attention_mask', 'labels'],
        num_rows: 19952
    })
})
Training set size: 15961
Validation set size: 3991
{'problem': "Here's a way to construct a list containing every positive rational number:\n\nBuild a binary tree where each node is a rational and the root is `1/1`, with the following rules for creating the nodes below:\n* The value of the left-hand node below `a/b` is `a/a+b` or `a+b/b`\n* The value of the right-hand node below `a/b` is `a/a+b` or `a+b/b`\n\nSo the tree will look like this:\n\n```\n                       1/1\n                  /           \\ \n            1/2                  2/1\n           /    \\              /     \\\n       1/3        3/2        2/3       3/1\n      /   \\      /   \\      /   \\     /   \\\n   1/4    4/3  3/5   5/2  2/5   5/3  3/4   4/1\n \n ...\n```\n\nNow traverse the tree, breadth first, to get a list of rationals.\n\n```\n[ 1/1, 1/2, 2/1, 1/3, 3/2, 2/3, 3/1, 1/4, 4/3, 3/5, 5/2, .. ]\n```\n\nEvery positive rational will occur, in its reduced form, exactly once in the list, at a finite index.\n\n```if:haskell\nIn the kata, we will use tuples of type `(Integer, Integer)` to represent rationals, where `(a, b)` represents `a / b`\n```\n```if:javascript\nIn the kata, we will use tuples of type `[ Number, Number ]` to represent rationals, where `[a,b]` represents `a / b`\n```\n\nUsing this method you could create an infinite list of tuples:\n\nmatching the list described above:\n\nHowever, constructing the actual list is too slow for our purposes. Instead, study the tree above, and write two functions:\n\nFor example:\n", 'answer': '1. What is the definition of "reduced form" for a rational number?\n2. What is the order in which the nodes are traversed in the breadth-first traversal?\n3. What is the expected output format for the list of rationals?', 'type': 'Ambiguous', 'input_ids': [1, 2266, 29915, 29879, 263, 982, 304, 3386, 263, 1051, 6943, 1432, 6374, 17903, 1353, 29901, 13, 13, 8893, 263, 7581, 5447, 988, 1269, 2943, 338, 263, 17903, 322, 278, 3876, 338, 421, 29896, 29914, 29896, 1673, 411, 278, 1494, 6865, 363, 4969, 278, 7573, 2400, 29901, 13, 29930, 450, 995, 310, 278, 2175, 29899, 3179, 2943, 2400, 421, 29874, 29914, 29890, 29952, 338, 421, 29874, 29914, 29874, 29974, 29890, 29952, 470, 421, 29874, 29974, 29890, 29914, 29890, 29952, 13, 29930, 450, 995, 310, 278, 1492, 29899, 3179, 2943, 2400, 421, 29874, 29914, 29890, 29952, 338, 421, 29874, 29914, 29874, 29974, 29890, 29952, 470, 421, 29874, 29974, 29890, 29914, 29890, 29952, 13, 13, 6295, 278, 5447, 674, 1106, 763, 445, 29901, 13, 13, 28956, 13, 462, 4706, 29896, 29914, 29896, 13, 462, 29871, 847, 965, 320, 29871, 13, 632, 29896, 29914, 29906, 462, 259, 29906, 29914, 29896, 13, 965, 847, 1678, 320, 795, 847, 268, 320, 13, 4706, 29896, 29914, 29941, 308, 29941, 29914, 29906, 308, 29906, 29914, 29941, 4706, 29941, 29914, 29896, 13, 418, 847, 259, 320, 418, 847, 259, 320, 418, 847, 259, 320, 268, 847, 259, 320, 13, 1678, 29896, 29914, 29946, 268, 29946, 29914, 29941, 259, 29941, 29914, 29945, 1678, 29945, 29914, 29906, 259, 29906, 29914, 29945, 1678, 29945, 29914, 29941, 259, 29941, 29914, 29946, 1678, 29946, 29914, 29896, 13, 29871, 13, 2023, 13, 28956, 13, 13, 10454, 29370, 278, 5447, 29892, 18423, 386, 937, 29892, 304, 679, 263, 1051, 310, 17903, 29879, 29889, 13, 13, 28956, 13, 29961, 29871, 29896, 29914, 29896, 29892, 29871, 29896, 29914, 29906, 29892, 29871, 29906, 29914, 29896, 29892, 29871, 29896, 29914, 29941, 29892, 29871, 29941, 29914, 29906, 29892, 29871, 29906, 29914, 29941, 29892, 29871, 29941, 29914, 29896, 29892, 29871, 29896, 29914, 29946, 29892, 29871, 29946, 29914, 29941, 29892, 29871, 29941, 29914, 29945, 29892, 29871, 29945, 29914, 29906, 29892, 6317, 4514, 13, 28956, 13, 13, 26526, 6374, 17903, 674, 6403, 29892, 297, 967, 12212, 883, 29892, 3721, 2748, 297, 278, 1051, 29892, 472, 263, 8093, 2380, 29889, 13, 13, 28956, 361, 29901, 29882, 21918, 13, 797, 278, 413, 532, 29892, 591, 674, 671, 5291, 2701, 310, 1134, 12270, 7798, 29892, 8102, 3569, 304, 2755, 17903, 29879, 29892, 988, 12270, 29874, 29892, 289, 3569, 11524, 421, 29874, 847, 289, 29952, 13, 28956, 13, 28956, 361, 29901, 7729, 13, 797, 278, 413, 532, 29892, 591, 674, 671, 5291, 2701, 310, 1134, 10338, 9681, 29892, 9681, 4514, 29952, 304, 2755, 17903, 29879, 29892, 988, 10338, 29874, 29892, 29890, 7961, 11524, 421, 29874, 847, 289, 29952, 13, 28956, 13, 13, 15156, 445, 1158, 366, 1033, 1653, 385, 10362, 1051, 310, 5291, 2701, 29901, 13, 13, 4352, 292, 278, 1051, 5439, 2038, 29901, 13, 13, 17245, 29892, 3386, 292, 278, 3935, 1051, 338, 2086, 5232, 363, 1749, 11976, 29889, 8669, 29892, 6559, 278, 5447, 2038, 29892, 322, 2436, 1023, 3168, 29901, 13, 13, 2831, 1342, 29901, 13, 29896, 29889, 1724, 338, 278, 5023, 310, 376, 9313, 1133, 883, 29908, 363, 263, 17903, 1353, 29973, 13, 29906, 29889, 1724, 338, 278, 1797, 297, 607, 278, 7573, 526, 13310, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1, 2266, 29915, 29879, 263, 982, 304, 3386, 263, 1051, 6943, 1432, 6374, 17903, 1353, 29901, 13, 13, 8893, 263, 7581, 5447, 988, 1269, 2943, 338, 263, 17903, 322, 278, 3876, 338, 421, 29896, 29914, 29896, 1673, 411, 278, 1494, 6865, 363, 4969, 278, 7573, 2400, 29901, 13, 29930, 450, 995, 310, 278, 2175, 29899, 3179, 2943, 2400, 421, 29874, 29914, 29890, 29952, 338, 421, 29874, 29914, 29874, 29974, 29890, 29952, 470, 421, 29874, 29974, 29890, 29914, 29890, 29952, 13, 29930, 450, 995, 310, 278, 1492, 29899, 3179, 2943, 2400, 421, 29874, 29914, 29890, 29952, 338, 421, 29874, 29914, 29874, 29974, 29890, 29952, 470, 421, 29874, 29974, 29890, 29914, 29890, 29952, 13, 13, 6295, 278, 5447, 674, 1106, 763, 445, 29901, 13, 13, 28956, 13, 462, 4706, 29896, 29914, 29896, 13, 462, 29871, 847, 965, 320, 29871, 13, 632, 29896, 29914, 29906, 462, 259, 29906, 29914, 29896, 13, 965, 847, 1678, 320, 795, 847, 268, 320, 13, 4706, 29896, 29914, 29941, 308, 29941, 29914, 29906, 308, 29906, 29914, 29941, 4706, 29941, 29914, 29896, 13, 418, 847, 259, 320, 418, 847, 259, 320, 418, 847, 259, 320, 268, 847, 259, 320, 13, 1678, 29896, 29914, 29946, 268, 29946, 29914, 29941, 259, 29941, 29914, 29945, 1678, 29945, 29914, 29906, 259, 29906, 29914, 29945, 1678, 29945, 29914, 29941, 259, 29941, 29914, 29946, 1678, 29946, 29914, 29896, 13, 29871, 13, 2023, 13, 28956, 13, 13, 10454, 29370, 278, 5447, 29892, 18423, 386, 937, 29892, 304, 679, 263, 1051, 310, 17903, 29879, 29889, 13, 13, 28956, 13, 29961, 29871, 29896, 29914, 29896, 29892, 29871, 29896, 29914, 29906, 29892, 29871, 29906, 29914, 29896, 29892, 29871, 29896, 29914, 29941, 29892, 29871, 29941, 29914, 29906, 29892, 29871, 29906, 29914, 29941, 29892, 29871, 29941, 29914, 29896, 29892, 29871, 29896, 29914, 29946, 29892, 29871, 29946, 29914, 29941, 29892, 29871, 29941, 29914, 29945, 29892, 29871, 29945, 29914, 29906, 29892, 6317, 4514, 13, 28956, 13, 13, 26526, 6374, 17903, 674, 6403, 29892, 297, 967, 12212, 883, 29892, 3721, 2748, 297, 278, 1051, 29892, 472, 263, 8093, 2380, 29889, 13, 13, 28956, 361, 29901, 29882, 21918, 13, 797, 278, 413, 532, 29892, 591, 674, 671, 5291, 2701, 310, 1134, 12270, 7798, 29892, 8102, 3569, 304, 2755, 17903, 29879, 29892, 988, 12270, 29874, 29892, 289, 3569, 11524, 421, 29874, 847, 289, 29952, 13, 28956, 13, 28956, 361, 29901, 7729, 13, 797, 278, 413, 532, 29892, 591, 674, 671, 5291, 2701, 310, 1134, 10338, 9681, 29892, 9681, 4514, 29952, 304, 2755, 17903, 29879, 29892, 988, 10338, 29874, 29892, 29890, 7961, 11524, 421, 29874, 847, 289, 29952, 13, 28956, 13, 13, 15156, 445, 1158, 366, 1033, 1653, 385, 10362, 1051, 310, 5291, 2701, 29901, 13, 13, 4352, 292, 278, 1051, 5439, 2038, 29901, 13, 13, 17245, 29892, 3386, 292, 278, 3935, 1051, 338, 2086, 5232, 363, 1749, 11976, 29889, 8669, 29892, 6559, 278, 5447, 2038, 29892, 322, 2436, 1023, 3168, 29901, 13, 13, 2831, 1342, 29901, 13, 29896, 29889, 1724, 338, 278, 5023, 310, 376, 9313, 1133, 883, 29908, 363, 263, 17903, 1353, 29973, 13, 29906, 29889, 1724, 338, 278, 1797, 297, 607, 278, 7573, 526, 13310, 2]}
{'problem': 'Greg has a weighed directed graph, consisting of n vertices. In this graph any pair of distinct vertices has an edge between them in both directions. Greg loves playing with the graph and now he has invented a new game:  The game consists of n steps.  On the i-th step Greg removes vertex number x_{i} from the graph. As Greg removes a vertex, he also removes all the edges that go in and out of this vertex.  Before executing each step, Greg wants to know the sum of lengths of the shortest paths between all pairs of the remaining vertices. The shortest path can go through any remaining vertex. In other words, if we assume that d(i, v, u) is the shortest path between vertices v and u in the graph that formed before deleting vertex x_{i}, then Greg wants to know the value of the following sum: $\\sum_{v, u, v \\neq u} d(i, v, u)$. \n\nHelp Greg, print the value of the required sum before each step.\n\n\n-----Input-----\n\nThe first line contains integer n (1 ≤ n ≤ 500) — the number of vertices in the graph.\n\nNext n lines contain n integers each — the graph adjacency matrix: the j-th number in the i-th line a_{ij} (1 ≤ a_{ij} ≤ 10^5, a_{ii} = 0) represents the weight of the edge that goes from vertex i to vertex j.\n\nThe next line contains n distinct integers: x_1, x_2, ..., x_{n} (1 ≤ x_{i} ≤ n) — the vertices that Greg deletes.\n\n\n-----Output-----\n\nPrint n integers — the i-th number equals the required sum before the i-th step.\n\nPlease, do not use the %lld specifier to read or write 64-bit integers in C++. It is preferred to use the cin, cout streams of the %I64d specifier.\n\n\n-----Examples-----\nInput\n1\n0\n1\n\nOutput\n0 \nInput\n2\n0 5\n4 0\n1 2\n\nOutput\n9 0 \nInput\n4\n0 3 1 1\n6 0 400 1\n2 4 0 1\n1 1 1 0\n4 1 2 3\n\nOutput\n17 23 404 0', 'answer': "import sys\nfrom array import array  # noqa: F401\n\nn = int(input())\nmatrix = [array('i', list(map(int, input().split()))) for _ in range(n)]\naa = tuple([int(x) - 1 for x in input().split()])\nans = [''] * n\n\nfor i in range(n-1, -1, -1):\n    x = aa[i]\n\n    for a in range(n):\n        for b in range(n):\n            if matrix[a][b] > matrix[a][x] + matrix[x][b]:\n                matrix[a][b] = matrix[a][x] + matrix[x][b]\n\n    val, overflow = 0, 0\n    for a in aa[i:]:\n        for b in aa[i:]:\n            val += matrix[a][b]\n        if val > 10**9:\n            overflow += 1\n            val -= 10**9\n\n    ans[i] = str(10**9 * overflow + val)\n\nprint(' '.join(ans))\n", 'type': 'Original', 'input_ids': [1, 12051, 756, 263, 591, 25398, 10624, 3983, 29892, 19849, 310, 302, 13791, 29889, 512, 445, 3983, 738, 5101, 310, 8359, 13791, 756, 385, 7636, 1546, 963, 297, 1716, 18112, 29889, 12051, 12355, 267, 8743, 411, 278, 3983, 322, 1286, 540, 756, 11817, 287, 263, 716, 3748, 29901, 29871, 450, 3748, 11624, 310, 302, 6576, 29889, 29871, 1551, 278, 474, 29899, 386, 4331, 12051, 25388, 12688, 1353, 921, 648, 29875, 29913, 515, 278, 3983, 29889, 1094, 12051, 25388, 263, 12688, 29892, 540, 884, 25388, 599, 278, 12770, 393, 748, 297, 322, 714, 310, 445, 12688, 29889, 29871, 10949, 14012, 1269, 4331, 29892, 12051, 10753, 304, 1073, 278, 2533, 310, 27497, 310, 278, 3273, 342, 10898, 1546, 599, 11000, 310, 278, 9886, 13791, 29889, 450, 3273, 342, 2224, 508, 748, 1549, 738, 9886, 12688, 29889, 512, 916, 3838, 29892, 565, 591, 5251, 393, 270, 29898, 29875, 29892, 325, 29892, 318, 29897, 338, 278, 3273, 342, 2224, 1546, 13791, 325, 322, 318, 297, 278, 3983, 393, 8429, 1434, 21228, 12688, 921, 648, 29875, 1118, 769, 12051, 10753, 304, 1073, 278, 995, 310, 278, 1494, 2533, 29901, 779, 2083, 648, 29894, 29892, 318, 29892, 325, 320, 10743, 318, 29913, 270, 29898, 29875, 29892, 325, 29892, 318, 4935, 29871, 13, 13, 29648, 12051, 29892, 1596, 278, 995, 310, 278, 3734, 2533, 1434, 1269, 4331, 29889, 13, 13, 13, 23648, 4290, 23648, 13, 13, 1576, 937, 1196, 3743, 6043, 302, 313, 29896, 29871, 30248, 302, 29871, 30248, 29871, 29945, 29900, 29900, 29897, 813, 278, 1353, 310, 13791, 297, 278, 3983, 29889, 13, 13, 9190, 302, 3454, 1712, 302, 11920, 1269, 813, 278, 3983, 12109, 562, 3819, 4636, 29901, 278, 432, 29899, 386, 1353, 297, 278, 474, 29899, 386, 1196, 263, 648, 823, 29913, 313, 29896, 29871, 30248, 263, 648, 823, 29913, 29871, 30248, 29871, 29896, 29900, 29985, 29945, 29892, 263, 648, 2236, 29913, 353, 29871, 29900, 29897, 11524, 278, 7688, 310, 278, 7636, 393, 5771, 515, 12688, 474, 304, 12688, 432, 29889, 13, 13, 1576, 2446, 1196, 3743, 302, 8359, 11920, 29901, 921, 29918, 29896, 29892, 921, 29918, 29906, 29892, 2023, 29892, 921, 648, 29876, 29913, 313, 29896, 29871, 30248, 921, 648, 29875, 29913, 29871, 30248, 302, 29897, 813, 278, 13791, 393, 12051, 7374, 267, 29889, 13, 13, 13, 23648, 6466, 23648, 13, 13, 11816, 302, 11920, 813, 278, 474, 29899, 386, 1353, 15743, 278, 3734, 2533, 1434, 278, 474, 29899, 386, 4331, 29889, 13, 13, 12148, 29892, 437, 451, 671, 278, 1273, 29880, 430, 1580, 3709, 304, 1303, 470, 2436, 29871, 29953, 29946, 29899, 2966, 11920, 297, 315, 1817, 29889, 739, 338, 16389, 304, 671, 278, 4670, 29892, 11196, 20873, 310, 278, 1273, 29902, 29953, 29946, 29881, 1580, 3709, 29889, 13, 13, 13, 23648, 1252, 9422, 23648, 13, 4290, 13, 29896, 13, 29900, 13, 29896, 13, 13, 6466, 13, 29900, 29871, 13, 4290, 13, 29906, 13, 29900, 29871, 29945, 13, 29946, 29871, 29900, 13, 29896, 29871, 29906, 13, 13, 6466, 13, 29929, 29871, 29900, 29871, 13, 4290, 13, 29946, 13, 29900, 29871, 29941, 29871, 29896, 29871, 29896, 13, 29953, 29871, 29900, 29871, 29946, 29900, 29900, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1, 12051, 756, 263, 591, 25398, 10624, 3983, 29892, 19849, 310, 302, 13791, 29889, 512, 445, 3983, 738, 5101, 310, 8359, 13791, 756, 385, 7636, 1546, 963, 297, 1716, 18112, 29889, 12051, 12355, 267, 8743, 411, 278, 3983, 322, 1286, 540, 756, 11817, 287, 263, 716, 3748, 29901, 29871, 450, 3748, 11624, 310, 302, 6576, 29889, 29871, 1551, 278, 474, 29899, 386, 4331, 12051, 25388, 12688, 1353, 921, 648, 29875, 29913, 515, 278, 3983, 29889, 1094, 12051, 25388, 263, 12688, 29892, 540, 884, 25388, 599, 278, 12770, 393, 748, 297, 322, 714, 310, 445, 12688, 29889, 29871, 10949, 14012, 1269, 4331, 29892, 12051, 10753, 304, 1073, 278, 2533, 310, 27497, 310, 278, 3273, 342, 10898, 1546, 599, 11000, 310, 278, 9886, 13791, 29889, 450, 3273, 342, 2224, 508, 748, 1549, 738, 9886, 12688, 29889, 512, 916, 3838, 29892, 565, 591, 5251, 393, 270, 29898, 29875, 29892, 325, 29892, 318, 29897, 338, 278, 3273, 342, 2224, 1546, 13791, 325, 322, 318, 297, 278, 3983, 393, 8429, 1434, 21228, 12688, 921, 648, 29875, 1118, 769, 12051, 10753, 304, 1073, 278, 995, 310, 278, 1494, 2533, 29901, 779, 2083, 648, 29894, 29892, 318, 29892, 325, 320, 10743, 318, 29913, 270, 29898, 29875, 29892, 325, 29892, 318, 4935, 29871, 13, 13, 29648, 12051, 29892, 1596, 278, 995, 310, 278, 3734, 2533, 1434, 1269, 4331, 29889, 13, 13, 13, 23648, 4290, 23648, 13, 13, 1576, 937, 1196, 3743, 6043, 302, 313, 29896, 29871, 30248, 302, 29871, 30248, 29871, 29945, 29900, 29900, 29897, 813, 278, 1353, 310, 13791, 297, 278, 3983, 29889, 13, 13, 9190, 302, 3454, 1712, 302, 11920, 1269, 813, 278, 3983, 12109, 562, 3819, 4636, 29901, 278, 432, 29899, 386, 1353, 297, 278, 474, 29899, 386, 1196, 263, 648, 823, 29913, 313, 29896, 29871, 30248, 263, 648, 823, 29913, 29871, 30248, 29871, 29896, 29900, 29985, 29945, 29892, 263, 648, 2236, 29913, 353, 29871, 29900, 29897, 11524, 278, 7688, 310, 278, 7636, 393, 5771, 515, 12688, 474, 304, 12688, 432, 29889, 13, 13, 1576, 2446, 1196, 3743, 302, 8359, 11920, 29901, 921, 29918, 29896, 29892, 921, 29918, 29906, 29892, 2023, 29892, 921, 648, 29876, 29913, 313, 29896, 29871, 30248, 921, 648, 29875, 29913, 29871, 30248, 302, 29897, 813, 278, 13791, 393, 12051, 7374, 267, 29889, 13, 13, 13, 23648, 6466, 23648, 13, 13, 11816, 302, 11920, 813, 278, 474, 29899, 386, 1353, 15743, 278, 3734, 2533, 1434, 278, 474, 29899, 386, 4331, 29889, 13, 13, 12148, 29892, 437, 451, 671, 278, 1273, 29880, 430, 1580, 3709, 304, 1303, 470, 2436, 29871, 29953, 29946, 29899, 2966, 11920, 297, 315, 1817, 29889, 739, 338, 16389, 304, 671, 278, 4670, 29892, 11196, 20873, 310, 278, 1273, 29902, 29953, 29946, 29881, 1580, 3709, 29889, 13, 13, 13, 23648, 1252, 9422, 23648, 13, 4290, 13, 29896, 13, 29900, 13, 29896, 13, 13, 6466, 13, 29900, 29871, 13, 4290, 13, 29906, 13, 29900, 29871, 29945, 13, 29946, 29871, 29900, 13, 29896, 29871, 29906, 13, 13, 6466, 13, 29929, 29871, 29900, 29871, 13, 4290, 13, 29946, 13, 29900, 29871, 29941, 29871, 29896, 29871, 29896, 13, 29953, 29871, 29900, 29871, 29946, 29900, 29900, 2]}
trainable params: 16777216 || all params: 6755323904 || trainable%: 0.24835546360798158
compiling the model
{'loss': 1.4021, 'grad_norm': 0.058257631957530975, 'learning_rate': 5e-05, 'epoch': 0.01}
{'loss': 1.3766, 'grad_norm': 0.10038375109434128, 'learning_rate': 0.0001, 'epoch': 0.02}
{'loss': 1.2933, 'grad_norm': 0.22992587089538574, 'learning_rate': 0.00015, 'epoch': 0.03}
{'loss': 1.2719, 'grad_norm': 0.24741750955581665, 'learning_rate': 0.0002, 'epoch': 0.04}
{'eval_loss': 1.1971893310546875, 'eval_runtime': 399.109, 'eval_samples_per_second': 10.0, 'eval_steps_per_second': 1.25, 'epoch': 0.04}
{'loss': 1.1123, 'grad_norm': 0.5726132392883301, 'learning_rate': 0.00025, 'epoch': 0.05}
{'loss': 1.2722, 'grad_norm': 0.12124724686145782, 'learning_rate': 0.0003, 'epoch': 0.06}
{'loss': 1.1748, 'grad_norm': 0.1223243847489357, 'learning_rate': 0.00035, 'epoch': 0.07}
{'loss': 1.0768, 'grad_norm': 0.15118008852005005, 'learning_rate': 0.0004, 'epoch': 0.08}
{'eval_loss': 1.1031994819641113, 'eval_runtime': 400.9307, 'eval_samples_per_second': 9.954, 'eval_steps_per_second': 1.245, 'epoch': 0.08}
{'loss': 1.0567, 'grad_norm': 0.2289787083864212, 'learning_rate': 0.00045000000000000004, 'epoch': 0.09}
{'loss': 0.9985, 'grad_norm': 0.3654036223888397, 'learning_rate': 0.0005, 'epoch': 0.1}
{'loss': 1.182, 'grad_norm': 0.11757702380418777, 'learning_rate': 0.00048333333333333334, 'epoch': 0.11}
{'loss': 1.1521, 'grad_norm': 0.098951056599617, 'learning_rate': 0.00046666666666666666, 'epoch': 0.12}
{'eval_loss': 1.0872613191604614, 'eval_runtime': 405.1127, 'eval_samples_per_second': 9.852, 'eval_steps_per_second': 1.232, 'epoch': 0.12}
{'loss': 1.026, 'grad_norm': 0.12752790749073029, 'learning_rate': 0.00045000000000000004, 'epoch': 0.13}
{'loss': 1.0356, 'grad_norm': 0.28346869349479675, 'learning_rate': 0.00043333333333333337, 'epoch': 0.14}
{'loss': 0.9657, 'grad_norm': 0.37022820115089417, 'learning_rate': 0.0004166666666666667, 'epoch': 0.15}
{'loss': 1.0988, 'grad_norm': 0.09533482044935226, 'learning_rate': 0.0004, 'epoch': 0.16}
{'eval_loss': 1.0814547538757324, 'eval_runtime': 400.6474, 'eval_samples_per_second': 9.961, 'eval_steps_per_second': 1.245, 'epoch': 0.16}
{'loss': 1.129, 'grad_norm': 0.09991803020238876, 'learning_rate': 0.00038333333333333334, 'epoch': 0.17}
{'loss': 0.9809, 'grad_norm': 0.1064578965306282, 'learning_rate': 0.00036666666666666667, 'epoch': 0.18}
{'loss': 0.9657, 'grad_norm': 0.18243318796157837, 'learning_rate': 0.00035, 'epoch': 0.19}
{'loss': 0.9285, 'grad_norm': 0.2881791591644287, 'learning_rate': 0.0003333333333333333, 'epoch': 0.2}
{'eval_loss': 1.064610481262207, 'eval_runtime': 398.9615, 'eval_samples_per_second': 10.003, 'eval_steps_per_second': 1.251, 'epoch': 0.2}
{'loss': 1.1422, 'grad_norm': 0.08605535328388214, 'learning_rate': 0.00031666666666666665, 'epoch': 0.21}
{'loss': 1.0883, 'grad_norm': 0.09363993257284164, 'learning_rate': 0.0003, 'epoch': 0.22}
{'loss': 1.0368, 'grad_norm': 0.11379242688417435, 'learning_rate': 0.00028333333333333335, 'epoch': 0.23}
{'loss': 0.9419, 'grad_norm': 0.1530437469482422, 'learning_rate': 0.0002666666666666667, 'epoch': 0.24}
{'eval_loss': 1.0400094985961914, 'eval_runtime': 406.8857, 'eval_samples_per_second': 9.809, 'eval_steps_per_second': 1.226, 'epoch': 0.24}
{'loss': 0.905, 'grad_norm': 0.3168713450431824, 'learning_rate': 0.00025, 'epoch': 0.25}
{'loss': 1.1429, 'grad_norm': 0.09278533607721329, 'learning_rate': 0.00023333333333333333, 'epoch': 0.26}
{'loss': 1.1146, 'grad_norm': 0.10169413685798645, 'learning_rate': 0.00021666666666666668, 'epoch': 0.27}
{'loss': 1.0159, 'grad_norm': 0.11471012234687805, 'learning_rate': 0.0002, 'epoch': 0.28}
{'eval_loss': 1.034731149673462, 'eval_runtime': 398.2983, 'eval_samples_per_second': 10.02, 'eval_steps_per_second': 1.253, 'epoch': 0.28}
{'loss': 0.9763, 'grad_norm': 0.14245183765888214, 'learning_rate': 0.00018333333333333334, 'epoch': 0.29}
{'loss': 0.905, 'grad_norm': 0.2993335723876953, 'learning_rate': 0.00016666666666666666, 'epoch': 0.3}
{'loss': 1.0708, 'grad_norm': 0.11830443888902664, 'learning_rate': 0.00015, 'epoch': 0.31}
{'loss': 1.0419, 'grad_norm': 0.10440968722105026, 'learning_rate': 0.00013333333333333334, 'epoch': 0.32}
{'eval_loss': 1.0341951847076416, 'eval_runtime': 398.044, 'eval_samples_per_second': 10.027, 'eval_steps_per_second': 1.254, 'epoch': 0.32}
{'loss': 0.9575, 'grad_norm': 0.1637745052576065, 'learning_rate': 0.00011666666666666667, 'epoch': 0.33}
{'loss': 0.9993, 'grad_norm': 0.1632213592529297, 'learning_rate': 0.0001, 'epoch': 0.34}
{'loss': 0.8858, 'grad_norm': 0.25561758875846863, 'learning_rate': 8.333333333333333e-05, 'epoch': 0.35}
{'loss': 1.1012, 'grad_norm': 0.10588215291500092, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.36}
{'eval_loss': 1.0274702310562134, 'eval_runtime': 401.643, 'eval_samples_per_second': 9.937, 'eval_steps_per_second': 1.242, 'epoch': 0.36}
{'loss': 1.0909, 'grad_norm': 0.09977716952562332, 'learning_rate': 5e-05, 'epoch': 0.37}
{'loss': 1.0044, 'grad_norm': 0.1177816241979599, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.38}
{'loss': 0.9425, 'grad_norm': 0.1711483746767044, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.39}
{'loss': 0.9365, 'grad_norm': 0.30814462900161743, 'learning_rate': 0.0, 'epoch': 0.4}
{'eval_loss': 1.0223853588104248, 'eval_runtime': 398.3222, 'eval_samples_per_second': 10.02, 'eval_steps_per_second': 1.253, 'epoch': 0.4}
{'train_runtime': 7831.5865, 'train_samples_per_second': 0.817, 'train_steps_per_second': 0.051, 'train_loss': 1.0699698853492736, 'epoch': 0.4}
{'eval_loss': 1.0223853588104248, 'eval_runtime': 398.048, 'eval_samples_per_second': 10.026, 'eval_steps_per_second': 1.254, 'epoch': 0.40090202956652465}


 Two things are infinite:  
The universe is infinite, and so are the possibilities.

You are given a string of characters, and you need to find the longest possible substring that can be formed by rearranging the characters in the string.

For
