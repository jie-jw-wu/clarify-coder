DatasetDict({
    train: Dataset({
        features: ['problem', 'answer', 'type'],
        num_rows: 19952
    })
})
DatasetDict({
    train: Dataset({
        features: ['problem', 'answer', 'type', 'input_ids', 'attention_mask', 'labels'],
        num_rows: 19952
    })
})
Training set size: 15961
Validation set size: 3991
{'problem': "Here's a way to construct a list containing every positive rational number:\n\nBuild a binary tree where each node is a rational and the root is `1/1`, with the following rules for creating the nodes below:\n* The value of the left-hand node below `a/b` is `a/a+b` or `a+b/b`\n* The value of the right-hand node below `a/b` is `a/a+b` or `a+b/b`\n\nSo the tree will look like this:\n\n```\n                       1/1\n                  /           \\ \n            1/2                  2/1\n           /    \\              /     \\\n       1/3        3/2        2/3       3/1\n      /   \\      /   \\      /   \\     /   \\\n   1/4    4/3  3/5   5/2  2/5   5/3  3/4   4/1\n \n ...\n```\n\nNow traverse the tree, breadth first, to get a list of rationals.\n\n```\n[ 1/1, 1/2, 2/1, 1/3, 3/2, 2/3, 3/1, 1/4, 4/3, 3/5, 5/2, .. ]\n```\n\nEvery positive rational will occur, in its reduced form, exactly once in the list, at a finite index.\n\n```if:haskell\nIn the kata, we will use tuples of type `(Integer, Integer)` to represent rationals, where `(a, b)` represents `a / b`\n```\n```if:javascript\nIn the kata, we will use tuples of type `[ Number, Number ]` to represent rationals, where `[a,b]` represents `a / b`\n```\n\nUsing this method you could create an infinite list of tuples:\n\nmatching the list described above:\n\nHowever, constructing the actual list is too slow for our purposes. Instead, study the tree above, and write two functions:\n\nFor example:\n", 'answer': '1. What is the definition of "reduced form" for a rational number?\n2. What is the order in which the nodes are traversed in the breadth-first traversal?\n3. What is the expected output format for the list of rationals?', 'type': 'Ambiguous', 'input_ids': [32013, 4888, 6, 82, 245, 1141, 276, 5580, 245, 1517, 8896, 1129, 4856, 18282, 1594, 25, 185, 185, 11521, 245, 10042, 5319, 1064, 1317, 4256, 317, 245, 18282, 285, 254, 4330, 317, 2220, 16, 14, 16, 63, 11, 365, 254, 1884, 6544, 327, 6814, 254, 8806, 2867, 25, 185, 9, 428, 1432, 280, 254, 2104, 12, 4560, 4256, 2867, 2220, 64, 14, 65, 63, 317, 2220, 64, 14, 64, 10, 65, 63, 409, 2220, 64, 10, 65, 14, 65, 63, 185, 9, 428, 1432, 280, 254, 1327, 12, 4560, 4256, 2867, 2220, 64, 14, 65, 63, 317, 2220, 64, 14, 64, 10, 65, 63, 409, 2220, 64, 10, 65, 14, 65, 63, 185, 185, 3127, 254, 5319, 540, 1066, 833, 437, 25, 185, 185, 10252, 185, 3081, 16, 14, 16, 185, 6549, 889, 3137, 357, 207, 185, 2481, 16, 14, 17, 9204, 17, 14, 16, 185, 3137, 889, 315, 357, 3462, 889, 251, 357, 185, 436, 16, 14, 18, 294, 18, 14, 17, 294, 17, 14, 18, 436, 18, 14, 16, 185, 730, 889, 243, 357, 730, 889, 243, 357, 730, 889, 243, 357, 251, 889, 243, 357, 185, 315, 16, 14, 19, 251, 19, 14, 18, 243, 18, 14, 20, 315, 20, 14, 17, 243, 17, 14, 20, 315, 20, 14, 18, 243, 18, 14, 19, 315, 19, 14, 16, 185, 207, 185, 3552, 185, 10252, 185, 185, 4375, 1355, 4641, 254, 5319, 11, 9891, 392, 1019, 11, 276, 748, 245, 1517, 280, 427, 335, 909, 13, 185, 185, 10252, 185, 58, 207, 16, 14, 16, 11, 207, 16, 14, 17, 11, 207, 17, 14, 16, 11, 207, 16, 14, 18, 11, 207, 18, 14, 17, 11, 207, 17, 14, 18, 11, 207, 18, 14, 16, 11, 207, 16, 14, 19, 11, 207, 19, 14, 18, 11, 207, 18, 14, 20, 11, 207, 20, 14, 17, 11, 10165, 6337, 185, 10252, 185, 185, 10410, 4856, 18282, 540, 5182, 11, 279, 891, 8489, 1017, 11, 5082, 2551, 279, 254, 1517, 11, 429, 245, 7593, 3750, 13, 185, 185, 10252, 351, 25, 71, 1974, 487, 185, 769, 254, 528, 747, 11, 394, 540, 931, 244, 393, 873, 280, 1443, 2220, 7, 12959, 11, 18782, 8, 63, 276, 2554, 427, 335, 909, 11, 1064, 2220, 7, 64, 11, 270, 8, 63, 9622, 2220, 64, 889, 270, 63, 185, 10252, 185, 10252, 351, 25, 9611, 185, 769, 254, 528, 747, 11, 394, 540, 931, 244, 393, 873, 280, 1443, 2220, 58, 11988, 11, 11988, 6337, 63, 276, 2554, 427, 335, 909, 11, 1064, 2220, 58, 64, 11, 65, 60, 63, 9622, 2220, 64, 889, 270, 63, 185, 10252, 185, 185, 11060, 437, 2040, 340, 1023, 2594, 274, 13158, 1517, 280, 244, 393, 873, 25, 185, 185, 10108, 272, 254, 1517, 5728, 2321, 25, 185, 185, 6502, 11, 5580, 272, 254, 4304, 1517, 317, 1646, 4351, 327, 764, 9996, 13, 12013, 11, 3421, 254, 5319, 2321, 11, 285, 3697, 979, 4900, 25, 185, 185, 1978, 2194, 25, 185, 16, 13, 2450, 317, 254, 6525, 280, 440, 504, 16599, 1017, 1, 327, 245, 18282, 1594, 30, 32021], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 13, 2450, 317, 254, 6525, 280, 440, 504, 16599, 1017, 1, 327, 245, 18282, 1594, 30, 32021]}
{'problem': 'Greg has a weighed directed graph, consisting of n vertices. In this graph any pair of distinct vertices has an edge between them in both directions. Greg loves playing with the graph and now he has invented a new game:  The game consists of n steps.  On the i-th step Greg removes vertex number x_{i} from the graph. As Greg removes a vertex, he also removes all the edges that go in and out of this vertex.  Before executing each step, Greg wants to know the sum of lengths of the shortest paths between all pairs of the remaining vertices. The shortest path can go through any remaining vertex. In other words, if we assume that d(i, v, u) is the shortest path between vertices v and u in the graph that formed before deleting vertex x_{i}, then Greg wants to know the value of the following sum: $\\sum_{v, u, v \\neq u} d(i, v, u)$. \n\nHelp Greg, print the value of the required sum before each step.\n\n\n-----Input-----\n\nThe first line contains integer n (1 ≤ n ≤ 500) — the number of vertices in the graph.\n\nNext n lines contain n integers each — the graph adjacency matrix: the j-th number in the i-th line a_{ij} (1 ≤ a_{ij} ≤ 10^5, a_{ii} = 0) represents the weight of the edge that goes from vertex i to vertex j.\n\nThe next line contains n distinct integers: x_1, x_2, ..., x_{n} (1 ≤ x_{i} ≤ n) — the vertices that Greg deletes.\n\n\n-----Output-----\n\nPrint n integers — the i-th number equals the required sum before the i-th step.\n\nPlease, do not use the %lld specifier to read or write 64-bit integers in C++. It is preferred to use the cin, cout streams of the %I64d specifier.\n\n\n-----Examples-----\nInput\n1\n0\n1\n\nOutput\n0 \nInput\n2\n0 5\n4 0\n1 2\n\nOutput\n9 0 \nInput\n4\n0 3 1 1\n6 0 400 1\n2 4 0 1\n1 1 1 0\n4 1 2 3\n\nOutput\n17 23 404 0', 'answer': "import sys\nfrom array import array  # noqa: F401\n\nn = int(input())\nmatrix = [array('i', list(map(int, input().split()))) for _ in range(n)]\naa = tuple([int(x) - 1 for x in input().split()])\nans = [''] * n\n\nfor i in range(n-1, -1, -1):\n    x = aa[i]\n\n    for a in range(n):\n        for b in range(n):\n            if matrix[a][b] > matrix[a][x] + matrix[x][b]:\n                matrix[a][b] = matrix[a][x] + matrix[x][b]\n\n    val, overflow = 0, 0\n    for a in aa[i:]:\n        for b in aa[i:]:\n            val += matrix[a][b]\n        if val > 10**9:\n            overflow += 1\n            val -= 10**9\n\n    ans[i] = str(10**9 * overflow + val)\n\nprint(' '.join(ans))\n", 'type': 'Original', 'input_ids': [32013, 38, 1928, 638, 245, 394, 311, 870, 13006, 4144, 11, 18382, 280, 291, 13631, 13, 680, 437, 4144, 683, 5689, 280, 8493, 13631, 638, 274, 5935, 1433, 763, 279, 1564, 13012, 13, 17219, 13666, 6066, 365, 254, 4144, 285, 1130, 362, 638, 1336, 8909, 245, 756, 2612, 25, 207, 428, 2612, 10675, 280, 291, 5598, 13, 207, 2416, 254, 460, 12, 392, 3443, 17219, 25174, 12078, 1594, 1371, 563, 72, 92, 473, 254, 4144, 13, 1725, 17219, 25174, 245, 12078, 11, 362, 835, 25174, 519, 254, 10769, 344, 610, 279, 285, 631, 280, 437, 12078, 13, 207, 12358, 24454, 1317, 3443, 11, 17219, 7573, 276, 1001, 254, 2545, 280, 25342, 280, 254, 2567, 370, 12921, 1433, 519, 12042, 280, 254, 9331, 13631, 13, 428, 2567, 370, 3076, 482, 610, 1182, 683, 9331, 12078, 13, 680, 746, 3061, 11, 562, 394, 6044, 344, 263, 7, 72, 11, 353, 11, 2631, 8, 317, 254, 2567, 370, 3076, 1433, 13631, 353, 285, 2631, 279, 254, 4144, 344, 9167, 1321, 29310, 12078, 1371, 563, 72, 1077, 930, 17219, 7573, 276, 1001, 254, 1432, 280, 254, 1884, 2545, 25, 371, 59, 1828, 563, 85, 11, 2631, 11, 353, 357, 9308, 2631, 92, 263, 7, 72, 11, 353, 11, 2631, 8, 3, 13, 207, 185, 185, 18542, 17219, 11, 3628, 254, 1432, 280, 254, 3414, 2545, 1321, 1317, 3443, 13, 185, 185, 185, 21647, 6546, 21647, 185, 185, 546, 1019, 1348, 5396, 10878, 291, 334, 16, 207, 156, 218, 97, 291, 207, 156, 218, 97, 207, 20, 15, 15, 8, 207, 1962, 254, 1594, 280, 13631, 279, 254, 4144, 13, 185, 185, 7926, 291, 4290, 3768, 291, 26682, 1317, 207, 1962, 254, 4144, 10648, 305, 1908, 5852, 25, 254, 521, 12, 392, 1594, 279, 254, 460, 12, 392, 1348, 245, 563, 4012, 92, 334, 16, 207, 156, 218, 97, 245, 563, 4012, 92, 207, 156, 218, 97, 207, 16, 15, 61, 20, 11, 245, 563, 3894, 92, 1412, 15, 8, 9622, 254, 4285, 280, 254, 5935, 344, 5002, 473, 12078, 460, 276, 12078, 521, 13, 185, 185, 546, 2100, 1348, 5396, 291, 8493, 26682, 25, 1371, 62, 16, 11, 1371, 62, 17, 11, 3552, 11, 1371, 563, 77, 92, 334, 16, 207, 156, 218, 97, 1371, 563, 72, 92, 207, 156, 218, 97, 291, 8, 207, 1962, 254, 13631, 344, 17219, 10166, 257, 13, 185, 185, 185, 21647, 8780, 21647, 185, 185, 16074, 291, 26682, 207, 1962, 254, 460, 12, 392, 1594, 21572, 254, 3414, 2545, 1321, 254, 460, 12, 392, 3443, 13, 185, 185, 7912, 11, 533, 441, 931, 254, 3018, 75, 402, 1300, 4664, 276, 1272, 409, 3697, 207, 21, 19, 12, 3374, 26682, 279, 339, 3868, 13, 804, 317, 12772, 276, 931, 254, 8380, 11, 27821, 21504, 280, 254, 3018, 40, 21, 19, 67, 1300, 4664, 13, 185, 185, 185, 21647, 1488, 10592, 21647, 185, 6546, 185, 16, 185, 15, 185, 16, 185, 185, 8780, 185, 15, 207, 185, 6546, 185, 17, 185, 15, 207, 20, 185, 19, 207, 15, 185, 16, 207, 17, 185, 185, 8780, 185, 24, 207, 15, 207, 185, 32021], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}
trainable params: 16777216 || all params: 6757289984 || trainable%: 0.24828320287756353
compiling the model
{'loss': 1.1695, 'grad_norm': 0.07599365711212158, 'learning_rate': 2.9999999999999997e-05, 'epoch': 0.08}
{'loss': 1.0653, 'grad_norm': 0.09816335886716843, 'learning_rate': 5.9999999999999995e-05, 'epoch': 0.16}
{'loss': 1.1315, 'grad_norm': 0.24172569811344147, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.24}
{'loss': 1.0721, 'grad_norm': 0.1547396183013916, 'learning_rate': 0.00011999999999999999, 'epoch': 0.32}
{'loss': 0.7575, 'grad_norm': 0.13658921420574188, 'learning_rate': 0.00015, 'epoch': 0.4}
{'loss': 0.5992, 'grad_norm': 0.15131445229053497, 'learning_rate': 0.00017999999999999998, 'epoch': 0.48}
{'loss': 0.7805, 'grad_norm': 0.2146845906972885, 'learning_rate': 0.00020999999999999998, 'epoch': 0.56}
{'loss': 0.6618, 'grad_norm': 0.12790782749652863, 'learning_rate': 0.00023999999999999998, 'epoch': 0.64}
{'loss': 0.5366, 'grad_norm': 0.12167127430438995, 'learning_rate': 0.00027, 'epoch': 0.72}
{'loss': 0.6926, 'grad_norm': 0.20514461398124695, 'learning_rate': 0.0003, 'epoch': 0.8}
{'eval_loss': 0.6134281754493713, 'eval_runtime': 403.4655, 'eval_samples_per_second': 9.892, 'eval_steps_per_second': 1.237, 'epoch': 0.8}
{'loss': 0.6204, 'grad_norm': 0.11362303048372269, 'learning_rate': 0.00029, 'epoch': 0.88}
{'loss': 0.5118, 'grad_norm': 0.10367146879434586, 'learning_rate': 0.00028, 'epoch': 0.96}
{'loss': 0.614, 'grad_norm': 0.1763095110654831, 'learning_rate': 0.00027, 'epoch': 1.04}
{'loss': 0.627, 'grad_norm': 0.10093281418085098, 'learning_rate': 0.00026, 'epoch': 1.12}
{'loss': 0.5112, 'grad_norm': 0.0838526040315628, 'learning_rate': 0.00025, 'epoch': 1.2}
{'loss': 0.5741, 'grad_norm': 0.156452015042305, 'learning_rate': 0.00023999999999999998, 'epoch': 1.28}
{'loss': 0.6571, 'grad_norm': 0.12841260433197021, 'learning_rate': 0.00023, 'epoch': 1.36}
{'loss': 0.4899, 'grad_norm': 0.09876980632543564, 'learning_rate': 0.00021999999999999995, 'epoch': 1.44}
{'loss': 0.5157, 'grad_norm': 0.1397082358598709, 'learning_rate': 0.00020999999999999998, 'epoch': 1.52}
{'loss': 0.6687, 'grad_norm': 0.13620032370090485, 'learning_rate': 0.00019999999999999998, 'epoch': 1.6}
{'eval_loss': 0.5146873593330383, 'eval_runtime': 404.2143, 'eval_samples_per_second': 9.873, 'eval_steps_per_second': 1.234, 'epoch': 1.6}
{'loss': 0.4823, 'grad_norm': 0.10952488332986832, 'learning_rate': 0.00018999999999999998, 'epoch': 1.68}
{'loss': 0.473, 'grad_norm': 0.1243017390370369, 'learning_rate': 0.00017999999999999998, 'epoch': 1.76}
{'loss': 0.6538, 'grad_norm': 0.15719866752624512, 'learning_rate': 0.00016999999999999999, 'epoch': 1.84}
{'loss': 0.4903, 'grad_norm': 0.11125434935092926, 'learning_rate': 0.00015999999999999999, 'epoch': 1.92}
{'loss': 0.4411, 'grad_norm': 0.1335391104221344, 'learning_rate': 0.00015, 'epoch': 2.0}
{'loss': 0.6168, 'grad_norm': 0.1086331382393837, 'learning_rate': 0.00014, 'epoch': 2.08}
{'loss': 0.5016, 'grad_norm': 0.11181273311376572, 'learning_rate': 0.00013, 'epoch': 2.16}
{'loss': 0.4208, 'grad_norm': 0.1043747067451477, 'learning_rate': 0.00011999999999999999, 'epoch': 2.24}
{'loss': 0.5907, 'grad_norm': 0.15109167993068695, 'learning_rate': 0.00010999999999999998, 'epoch': 2.32}
{'loss': 0.4886, 'grad_norm': 0.11079911887645721, 'learning_rate': 9.999999999999999e-05, 'epoch': 2.4}
{'eval_loss': 0.48369550704956055, 'eval_runtime': 405.4077, 'eval_samples_per_second': 9.844, 'eval_steps_per_second': 1.231, 'epoch': 2.4}
{'loss': 0.4274, 'grad_norm': 0.10703808814287186, 'learning_rate': 8.999999999999999e-05, 'epoch': 2.48}
{'loss': 0.5833, 'grad_norm': 0.1450781375169754, 'learning_rate': 7.999999999999999e-05, 'epoch': 2.57}
{'loss': 0.4864, 'grad_norm': 0.10956472158432007, 'learning_rate': 7e-05, 'epoch': 2.65}
{'loss': 0.4246, 'grad_norm': 0.11251053959131241, 'learning_rate': 5.9999999999999995e-05, 'epoch': 2.73}
{'loss': 0.5603, 'grad_norm': 0.18531562387943268, 'learning_rate': 4.9999999999999996e-05, 'epoch': 2.81}
{'loss': 0.5143, 'grad_norm': 0.10369010269641876, 'learning_rate': 3.9999999999999996e-05, 'epoch': 2.89}
{'loss': 0.4289, 'grad_norm': 0.10328143835067749, 'learning_rate': 2.9999999999999997e-05, 'epoch': 2.97}
{'loss': 0.4944, 'grad_norm': 0.16059572994709015, 'learning_rate': 1.9999999999999998e-05, 'epoch': 3.05}
{'loss': 0.4898, 'grad_norm': 0.09821169078350067, 'learning_rate': 9.999999999999999e-06, 'epoch': 3.13}
{'loss': 0.4289, 'grad_norm': 0.11239571869373322, 'learning_rate': 0.0, 'epoch': 3.21}
{'eval_loss': 0.47239840030670166, 'eval_runtime': 404.5662, 'eval_samples_per_second': 9.865, 'eval_steps_per_second': 1.233, 'epoch': 3.21}
{'train_runtime': 22377.8699, 'train_samples_per_second': 2.288, 'train_steps_per_second': 0.018, 'train_loss': 0.6063382065296173, 'epoch': 3.21}
{'eval_loss': 0.47239840030670166, 'eval_runtime': 401.8777, 'eval_samples_per_second': 9.931, 'eval_steps_per_second': 1.242, 'epoch': 3.2064128256513027}


 Two things are infinite: , and the number of ways to choose a subset of the set of all things.

































