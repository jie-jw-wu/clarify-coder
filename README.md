# clarify-aware-coder

## Data Generation Pipeline

### Datasets
#### APPS Dataset
- The APPS dataset consists of problems collected from different open-access coding websites such as Codeforces, Kattis, and more. The problems range in difficulty from introductory to collegiate competition level and measure coding ability as well as problem-solving. 
- The Automated Programming Progress Standard, abbreviated APPS, consists of 10,000 coding problems in total, with 131,836 test cases for checking solutions and 232,444 ground-truth solutions written by humans. Problems can be complicated, as the average length of a problem is 293.2 words. The data are split evenly into training and test sets, with 5,000 problems each. In the test set, every problem has multiple test cases, and the average number of test cases is 21.2. Each test case is specifically designed for the corresponding problem, enabling us to rigorously evaluate program functionality.
- Download the APPS dataset from the following link: https://people.eecs.berkeley.edu/~hendrycks/APPS.tar.gz
- The folder structure:
```
APPS
├── train
│   ├── 0000
│   │   ├── input_output.json
│   │   ├── metadata.json
│   │   ├── question.txt
│   │   └── solutions.json
│   └── ...
└── test
    ├── 0000
    │   ├── input_output.json
    │   ├── metadata.json
    │   ├── question.txt
    │   └── solutions.json
    └── ...

```

### LLMs
#### Gemini
- We use Google Gemini to generate our dataset. Why? Great performance and free access to API for research purposes.
- As such, we use imports such as google.generativeai and google.api_core.exceptions to handle errors such as ResourceExhausted.
- The API_KEY can be generated from the following link: https://aistudio.google.com/app/apikey. Please note that you will need a Google Account to get access to an API key.
- Current Configuration Settings:
    - Temperature: 0.1
    - Top_p: 1
    - Top_k: 1
    - Maximum output tokens: 2048 (is this enough?)
    - Safety settings set to “BLOCK_NONE”.

#### Step 1
- **NOTE: Currently, for preparation of the dataset, the authors have decided to use the `solutions.json` as ground truth code for the original problem description. This step can be omitted unless it is desirable to use LLM generated code for the fine-tuning process.**
- Generates code from the original problem statement.
- We use a zeroshot prompt to generate python code from the given problem description from the APPS dataset. 
```
You are given a coding problem description. Your task is to write the corresponding Python code to solve the problem. Follow the problem requirements carefully and ensure your code is efficient and correct. Here is the problem description:
{question}
Please provide the complete Python code below:
```
- The output from this is a JSONL file that contains input-output pairs, the input is the problem description from the APPS dataset and the output is the python code generated by the model.

#### Step 2

- Generates modified problem from the original problem statement.
- Reads the question.txt file from each problem folder from the dataset, puts it in the {question} placeholder in the prompt template.
- To generate the modified problem, we optimize our prompt to be a combination of Knowledge prompt as well as Chain-of-Thought.
- There are currently three kinds of problems we want to generate:
    - Ambiguous
    - Inconsistent
    - Incomplete
- The template for each can be found in the code and can be interchanged as seen fit.
- The output from this is a JSONL file that contains input-output pairs, the input is the problem description from the APPS dataset and the output is the modified (ambiguous/inconsistent/incomplete based on the prompt chosen) problem description generated by the model.

### Step 2.5

- To use output generated in step 2 for step 3, first the "output" present in the JSONL file needs to be converted to `txt` format. Use the `convert_JSONL_2_txt.py` file to do the same. The file creates folder (each folder's name is index number of JSONL row) and inside each folder will be a `modified_question.txt`. We will use this set of folders as the set of inputs for the next step.

```
Modified_Problems
├── 0
│   └── modified_question.txt
├── 1
│   └── modified_question.txt
└── ...

```

#### Step 3

- Generates clarifying questions and clarification score from the modified problems.
- Clarification Score: A measure of how necessary it is to ask clarifying questions to complete the coding task. A score of 0 means no clarifying questions are needed, while a score of 1 means clarifying questions are absolutely necessary. Default value for the original (not modified) problem descriptions is 0.0.
    - UPDATE: Not generating clarification score anymore.
    - Will be tagging the question as Ambiguous/Inconsistent/Incomplete
- The output from this is a JSONL file that contains input-output pairs, the input is the modified problem description and the output is the clarifying questions and the clarification score.

#### Usage for Step 1, Step 2, Step 3

The usage across the first three steps remain the same. As such, only the template changes. Here are the command line arguments:

```
--api_key: API key for Google Generative AI.
--dir_path: Directory containing folders with coding problems (example: APPS/train).
--json_file_path: Path to save the output JSONL file (example: output.jsonl).
```

Example Usage:

```
python script.py --api_key YOUR_API_KEY --dir_path path/to/your/directory --json_file_path path/to/your/output.json
```

#### Step 4

- This python script combines the data generated from step 1 and step 3 to create the finetuned dataset with the following entries:
    - Original Problem: Code (10k entries)
    - Ambiguous Problem: Clarifying Questions (10K entries)
    - Inconsistent Problem: Clarifying Questions (10K entries)
    - Incomplete Problem: Clarifying Questions (10K entries)
- Currently there will be 2 columns in this dataset (wtih scope to add more):
    - Problem Description: Either the original problem description from the dataset, or the modified problem description.
    - Output: Either code (ground truth) or clarifying questions with tag of what kind of problem it is (Ambiguous/Incomplete/Inconsistent)
    